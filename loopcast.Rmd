---
title: "Loop Cast"
output: html_document
author: Jordan Wente
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidyquant)
library(imputeTS)
library(rnoaa)
library(dygraphs)
library(lubridate)
library(magrittr)
library(xts)
library(betareg)
library(timeDate)
#remotes::install_github("fnoorian/mltsp")
library(mltsp)
library(e1071)


```

```{r data load,include=FALSE,show=FALSE}

data<-read_csv("loop2.csv")

summary(data)
#data$recs<-seq(1,length(data$`Virtual.zzXcel_Total#Block Demand Real Power#kW`))
#data[data$recs %in% errs,]


data[is.na(data[,2]),2]<-data[is.na(data[,2]),3]

data$Timestamp %<>% mdy_hm() 

#errs<-data[is.na(data$Timestamp),4]


names(data)<-c("time","block","real")   #all units in kW

data$block %<>% na_interpolation()  #linear interpolation for the remaining datapoints that don't print

#tz(data$time)<-"America/Chicago"  Not necessary 
#apply.hourly <- function(x, FUN,...) {
#  ep <- endpoints(x, 'hours')
#  period.apply(x, ep, FUN, ...)
#}

```


First, I will need to create exogenous variables like holidays etc..





```{r time series}

ys<- data$time %>% year() %>% as.factor %>% levels() %>% as.numeric()
hols<- ys %>%  holidayNYSE %>% as.Date              #Using the NYSE as a hoilday database. 

data$hour<-hour(data$time)
data$date<-as.Date(data$time)
data$block<-data$block*.25    #converting to kWh


#convert to hourly time series

data2<- data %>% group_by(hour,date) %>% summarise_at("block",sum) %>% mutate(time=paste0(date," ",hour,":","00")) %>% mutate_at("time",ymd_hm) %>% arrange(time) 

data2$weekday<-data2$time %>% isWeekday() %>% as.numeric
data2$holiday<-0
data2$holiday[as.Date(data2$time) %in% hols]<-1  # These seems to work pretty well. 

```


```{r weather data, include=FALSE}
#Run the "functions": script to get the hourly weather data function, data is downloaded from: http://mesonet.agron.iastate.edu/request/download.phtml?network=MN_ASOS
#Made this other function because it was too heavy and I just needed something simpler. 


tz_data<-"America%2FChicago" #TZ - get it in the TZ you want. 

td<-today()
#this function is simple and downloads data for a single year. Downloads the data to a temp file. 

msp.w<-getMSPw(station="MSP",2013,01,01,year2=year(td),month2=month(td),day2=day(td),tz_data = tz_data)

msp.w$valid %<>% as.character()
msp.w2<-msp.w[is.na(msp.w$tmpf)==F,]
msp.w<-NULL      

```

```{r write and merge weather file, include=FALSE}
msp.w2$valid %<>% as.character() %>% ymd_hm()

msp.w2$date<-as.Date(msp.w2$valid)
msp.w2$hour<-hour(msp.w2$valid)
vars<-c("tmpf","dwpf","p01i","vsby","sknt")

#need to ensure the variables are converted to numeric first. 
na_inter<-function(x){
 p<-as.numeric(x) 
 a<-na_interpolation(p)
 return(a)
}
  
msp.w2<- msp.w2 %>% mutate_at(vars,na_inter) %>% group_by(hour,date) %>% summarize_at(vars,mean,na.rm=T) %>% ungroup()



```


```{r timeseries creation}
library(tseries)
              
d2<-left_join(data2,msp.w2) %>% mutate_at(vars,na_inter) %>% ungroup()


#d2$ts<-msts(d2$block, seasonal.periods=c(24, 7*24, 365*24))
#attr(d2$ts,"frequency")<-24

d2$week<- week(d2$time) %>% as.factor()
d2$month<-month(d2$time) %>% as.factor()
d2$year<-year(d2$time) %>% as.factor()
d2$day<-day(d2$time)

d2$weekday<-ifelse(d2$holiday==1,0,d2$weekday)

peaks<- d2 %>% group_by(month,year) %>% summarize_at("block",max) 
names(peaks)<-c("month","year","peak")

d2<-left_join(d2,peaks)
d2$bln<-d2$block/d2$peak

#d2$bts<- as.xts(d2$bln,order.by=d2$time,frequency=24)
#Cutting down the training 
#arima<-c(seq(1,12)) %>% enframe() %>% mutate(train=rep(list(d2[d2$date>"2017-10-03" & d2$date<"2018-10-02",]))) %>% mutate(test=rep(list(d2[ d2$date>"2018-10-02",])))
  

train<-d2[d2$date>"2017-10-03" & d2$date<"2018-10-02",] %>% group_by(month) %>% nest() 
test<-d2[d2$date>"2018-10-02",] %>% group_by(month) %>% nest() 
names(test)<-c("month","test")
mods<-left_join(train,test)

msts_fn<-function(df){          #This function assumes that the data has already been split by month. 
 ds<-max(df$day) #get the maximum number of days
  y<-msts(df$block, seasonal.periods=c(24,7*24))
  df$msts1<-y
  return(df)
}

mods<- mods %>% mutate(data=map(data,msts_fn))

#Testing the tbats model

arima_fn<-function(df){
  xreg1=as.matrix(df$weekday)          
  auto.arima(df$msts1,xreg=xreg1)              
}


############################## linear models ######################

#mod1<-mods$data[[1]] %>% arima_fn()


#mods<-mods %>% mutate(arima=map(data,arima_fn))
#Very long load time, so saved a copy to avoid re-fitting the arimas. 
#(mods,file="mods_with_arima.RData")


############### regression tree ##################



```

TBATS model (Exponential smoothing state space model with Box-Cox transformation, ARMA errors, Trend and Seasonal components). In theory, this should be the ideal catch all simple model. The TBATS is good at handling multiple seasonalities in the time series - which clearly captiol loop load has. Also, we should be able to extract elements from the tbats and then run random forest forecasts with the output. 

```{r tbats model}
tbats.fn<-function(df){
  dt<-df$msts1
  tbats(dt,use.arma.errors = T)
}

test<-mods$data[[1]]$msts1 %>% mstl() %>% autoplot()  #This is a much better decomposition.

mstl_fn<-function(df){
  dt<-df$msts1
  mstl(dt)
}
  
#mods<-mods %>% mutate(tbats.mod=map(data,tbats.fn))

mods<-mods %>% mutate(arimax1=map(data,arima_fn))

mods$month %<>% as.character() %>% as.numeric()

mods<-mods %>% mutate(mstl1=map(data,mstl_fn)) %>% arrange(month)



#gg.dcom<-lapply(mods$mstl1,autoplot) 
#gg.dcom

forecast(mods$arimax1[[10]],h=744,xreg=as.matrix(mods$test[[10]][,5])) %>% cbind.data.frame(mods$test[[10]]$block) %>% write.csv("test.csv")



```

```{r trend and random forest}
#Write a function to decompose the time series and then get the trend, use random forest to 
######################.

trend_fn<-function(df){
  x<-stl(df$ts,s.window="periodic",robust=T)
  xreg1=as.matrix(df[,5])
  y<-x$time.series
  x2<-ts(y[,2]) %>% auto.arima(xreg=xreg1)
}

dec_fn<-function(df){
 stl(df$ts,s.window="periodic",robust=T) 
}

msts_fn<-function(df){
 msts(df$block,seasonal.periods = c(24,24*7)) 
}

fourmap<-function(df){
 fourier(df,K=c(2,2)) 
}


mods<- mods %>% mutate(trends=map(data,trend_fn)) %>% mutate(decom_ts=map(data,dec_fn)) %>% mutate(msts1=map(data,msts_fn)) %>% mutate(fuur=map(msts1,fourmap))



#N <- nrow(data_train)
#window <- (N / period) - 1
#
#mods<- mods %>% mutate(fuur=)


#resids<-mods
  
  
#  mods$month %>% enframe(name=NULL) %>% 

#test<-cbind(mods$arima[[1]]$x,mods$arima[[1]]$fitted)




#Little to no relationship between temperature and block demand, it should seem. 

#arima model
#suggests an arima(0,1,5 model)
#arimax1<-mods$train[[3]]$ts %>% log() %>%  auto.arima(xreg=as.matrix(mods$train[[3]][,5:6]))


#fit an auto-arima model. 



```



Alright, got my modeling framework set up. now let's construct the model functions. Starting with the classical Box-Jenkins approach among others.

From literature review: ANN with Bayesian Regulation backpropagation for peak forecasts and 
ARX with Bayesian Regulation back-propagation showed the best overall performance on day ahead hourly.

The nonlinear autoregressive network with exogenous inputs(NARX) is a mixture of Neural Network and Time Series methodsand is based on the linear Auto Regressive Model with Exogenousinputs (ARX)

```{r small train set}



```



```{r model functions}
#may want to break up model up. HOw many differences to get the time series stationary.

ndiffs(log(mods$train[[1]]$ts))  #apparently it's only one. 

# Beta regression
beta_reg_fn<-function(x){
      x$bln[x$bln==1]<-.9999999     #a super high probability of hitting peak demand
      f<-formula(bln~month+hour+tmpf +dwpf + p01i + vsby + sknt + holiday + weekday)
     fit<-betareg(data=x,formula=f,link="logit")
}


beta_reg<-mods$train[[1]] %>% beta_reg_fn()

ah=48
pp <- list(list("diff", "auto"))
fx <- function(x) cbind(x, lag_windows(x, p = 168))  #360 hour length variables
ln <- SimpleLM
fcster <- mltsp_forecaster(pp, fx, ln)
narx1<-fcster

res<-fcster(mods$train[[1]]$ts, h=48)

vals<-merge.xts(res,mods$test[[1]]$ts[1:ah])
dygraph(vals)

#What's the mape?
mape(vals[,2],vals[,1])   #3.34% mean percent error


```




